replay buffer 1000000 [412] 1695
shape (1000000, 412) (1000000, 412)
---------------------MINIBATCH 1 -----------------
appending prev_obs with size: 0
running game for steps:  4
observation:  [[0 1 0 ... 1 0 0]
 [0 1 0 ... 1 0 0]
 [0 1 0 ... 1 0 0]]
possible_actions_probablities [[-30.75562477  31.00727654 -34.02735901 ...         -inf         -inf
  121.83539581]
 [-95.63748932  31.40052414 -78.79717255 ...         -inf         -inf
  116.34811401]
 [ 44.7438736   34.52290726 -60.91156769 ...         -inf         -inf
  120.15318298]]
actions taken:  [1694    7   12]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
observation:  [[1 0 0 ... 0 1 0]
 [1 0 0 ... 1 0 0]
 [1 0 0 ... 1 0 0]]
possible_actions_probablities [[-131.57655334   74.75521851  181.82971191 ...          -inf
           -inf  143.0934906 ]
 [         -inf          -inf          -inf ...          -inf
           -inf  131.65901184]
 [         -inf          -inf          -inf ...          -inf
           -inf  152.76644897]]
actions taken:  [   2    9 1694]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
observation:  [[1 0 0 ... 1 0 0]
 [1 0 0 ... 1 0 0]
 [1 0 0 ... 0 1 0]]
possible_actions_probablities [[        -inf         -inf -16.95789337 ...         -inf         -inf
   94.49662018]
 [        -inf         -inf         -inf ...         -inf         -inf
  126.54058838]
 [        -inf         -inf         -inf ...         -inf         -inf
  154.20497131]]
actions taken:  [   7   12 1694]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
observation:  [[0 1 0 ... 1 0 0]
 [0 1 0 ... 1 0 0]
 [0 1 0 ... 0 0 1]]
possible_actions_probablities [[       -inf        -inf        -inf ...        -inf        -inf
  80.74602509]
 [       -inf        -inf        -inf ...        -inf        -inf
  73.31668091]
 [       -inf        -inf        -inf ...        -inf        -inf
  21.94024658]]
actions taken:  [1694 1694 1694]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
end length:  0
before cutting:  (4, 3, 412)
after cutting:  (4, 3, 412)
minibatch size 12
minibatch produced ->  (12, 412) (12, 1695) (12,) (12,) (12, 1, 412) (12,)
remembering minibatch
learning params
reward:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
q1_old_policy, q_hat tensor([ 0.1523,  0.5059, 86.1961, 86.1944,  0.1523,  0.6392,  0.5059,  0.3708,
         0.6245, 86.2311, 86.1961, 86.1827], grad_fn=<ViewBackward>) tensor([1.9887, 1.9932, 1.9913, 1.9722, 1.9887, 1.9834, 1.9932, 1.9682, 1.9840,
        2.0008, 1.9913, 1.9854], grad_fn=<AddBackward0>)
critic_loss:  tensor(5232.2773, grad_fn=<AddBackward0>)
---------------------MINIBATCH 2 -----------------
appending prev_obs with size: 4
running game for steps:  4
observation:  [[0 1 0 ... 0 1 0]
 [0 1 0 ... 0 1 0]
 [0 1 0 ... 0 0 0]]
possible_actions_probablities [[           -inf            -inf            -inf ...            -inf
             -inf  1.01129417e+02]
 [           -inf            -inf            -inf ...            -inf
             -inf  1.98763611e+02]
 [-1.11379521e-03 -1.00731239e+01 -1.40930405e+02 ...            -inf
             -inf            -inf]]
actions taken:  [  12 1694  113]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
observation:  [[1 0 0 ... 1 0 0]
 [1 0 0 ... 0 0 1]
 [1 0 0 ... 1 0 0]]
possible_actions_probablities [[        -inf         -inf         -inf ...         -inf         -inf
   94.4715271 ]
 [        -inf         -inf         -inf ...         -inf         -inf
   18.78142929]
 [        -inf         -inf         -inf ...         -inf         -inf
  167.87913513]]
actions taken:  [1694 1694 1694]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
observation:  [[1 0 0 ... 0 1 0]
 [1 0 0 ... 0 0 0]
 [1 0 0 ... 0 1 0]]
possible_actions_probablities [[        -inf         -inf         -inf ...         -inf         -inf
  169.69721985]
 [-51.16410828  66.06096649  81.74710083 ...         -inf         -inf
          -inf]
 [        -inf         -inf         -inf ...         -inf         -inf
  167.46122742]]
actions taken:  [1694  356 1694]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
observation:  [[0 1 0 ... 0 0 1]
 [0 1 0 ... 1 0 0]
 [0 1 0 ... 0 0 1]]
possible_actions_probablities [[       -inf        -inf        -inf ...        -inf        -inf
  29.20750809]
 [       -inf        -inf        -inf ...        -inf        -inf
  69.85455322]
 [       -inf        -inf        -inf ...        -inf        -inf
  75.97725677]]
actions taken:  [1694 1694 1694]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
end length:  4
before cutting:  (4, 3, 412)
after cutting:  (4, 3, 412)
minibatch size 12
minibatch produced ->  (12, 412) (12, 1695) (12,) (12,) (12, 1, 412) (12,)
remembering minibatch
learning params
reward:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
q1_old_policy, q_hat tensor([-883.4406,   -1.0291,   -4.6803,  -58.9148, -883.4333,   -6.2425,
        -883.4379,   -6.2425,   -6.2496, -883.4379,   -4.6803, -883.4379],
       grad_fn=<ViewBackward>) tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       grad_fn=<AddBackward0>)
critic_loss:  tensor(nan, grad_fn=<AddBackward0>)
---------------------MINIBATCH 3 -----------------
appending prev_obs with size: 0
running game for steps:  4
observation:  [[0 1 0 ... 0 0 0]
 [0 1 0 ... 0 1 0]
 [0 1 0 ... 0 0 0]]
possible_actions_probablities [[ 71.71485138 -54.69046783 -45.09190369 ...         -inf         -inf
          -inf]
 [        -inf         -inf         -inf ...         -inf         -inf
  186.73094177]
 [204.1375885    7.31096029  90.72852325 ...         -inf         -inf
          -inf]]
actions taken:  [ 158 1694    0]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
observation:  [[1 0 0 ... 1 0 0]
 [1 0 0 ... 0 0 1]
 [1 0 0 ... 1 0 0]]
possible_actions_probablities [[        -inf         -inf         -inf ...         -inf         -inf
   89.17919159]
 [        -inf         -inf         -inf ...         -inf         -inf
  141.37075806]
 [        -inf         -inf -29.42179108 ...         -inf         -inf
  180.41809082]]
actions taken:  [ 323 1694    9]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
observation:  [[1 0 0 ... 1 0 0]
 [1 0 0 ... 0 0 0]
 [1 0 0 ... 1 0 0]]
possible_actions_probablities [[         -inf          -inf          -inf ...          -inf
           -inf  196.83094788]
 [  40.3800354    42.37557983 -132.62532043 ...          -inf
           -inf          -inf]
 [         -inf          -inf          -inf ...          -inf
           -inf  176.31414795]]
actions taken:  [1694    1 1694]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
observation:  [[0 1 0 ... 0 1 0]
 [0 1 0 ... 1 0 0]
 [0 1 0 ... 0 1 0]]
possible_actions_probablities [[        -inf         -inf         -inf ...         -inf         -inf
   51.87296295]
 [        -inf         -inf -34.84715271 ...         -inf         -inf
   89.83649445]
 [        -inf         -inf         -inf ...         -inf         -inf
  119.27099609]]
actions taken:  [1694    9    9]
rewards (array([0., 0., 0., 0.]), array([0., 0., 0., 0.]), array([0., 0., 0., 0.]))
end length:  0
before cutting:  (4, 3, 412)
after cutting:  (4, 3, 412)
minibatch size 12
minibatch produced ->  (12, 412) (12, 1695) (12,) (12,) (12, 1, 412) (12,)
remembering minibatch
learning params
reward:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
q1_old_policy, q_hat tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       grad_fn=<ViewBackward>) tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       grad_fn=<AddBackward0>)
critic_loss:  tensor(nan, grad_fn=<AddBackward0>)
---------------------MINIBATCH 4 -----------------
appending prev_obs with size: 4
running game for steps:  4
observation:  [[0 1 0 ... 0 0 1]
 [0 1 0 ... 1 0 0]
 [0 0 0 ... 1 0 0]]
