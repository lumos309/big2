{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The parameter loc has invalid values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_538/2068145223.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mmainSim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig2PPOSimulation\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnGames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnSteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipRange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mmainSim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time Taken: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_538/2068145223.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nTotalSteps)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# get minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mmb_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_availAcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_next_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_dones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'minibatch size'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnGames\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnSteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'minibatch produced -> '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_availAcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_next_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_dones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_538/2068145223.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# generate observatios from curstates and curavailacs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mneglogpacs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainingNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrStates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# add probabilities to the available actions, to rule out impossible actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/big2/sac_torch.py\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'observation: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreparameterize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/big2/networks.py\u001b[0m in \u001b[0;36msample_normal\u001b[0;34m(self, state, reparameterize)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreparameterize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreparameterize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn_course/lib/python3.7/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn_course/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip checking lazily-constructed args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The parameter {} has invalid values\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The parameter loc has invalid values"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#taken directly from baselines implementation - reshape minibatch in preparation for training.\n",
    "def sf01(arr):\n",
    "    \"\"\"\n",
    "    swap and then flatten axes 0 and 1\n",
    "    \"\"\"\n",
    "    s = arr.shape\n",
    "    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\n",
    "\n",
    "\n",
    "something = [1,1,1]\n",
    "from sac_torch import Agent\n",
    "from big2Game import vectorizedBig2Games\n",
    "\n",
    "global global_mb_reward\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False) \n",
    "\n",
    "\n",
    "class big2PPOSimulation(object):\n",
    "    \n",
    "    def __init__(self, *, inpDim = 412, nGames = 4, nSteps = 5, nMiniBatches = 4, nOptEpochs = 5, lam = 0.95, gamma = 0.995, ent_coef = 0.01, vf_coef = 0.5, max_grad_norm = 0.5, minLearningRate = 0.000001, learningRate, clipRange, saveEvery = 500):\n",
    "         \n",
    "        available_action_space =  1695\n",
    "        observation_space = [412]\n",
    "        \n",
    "        #environment\n",
    "        self.vectorizedGame = vectorizedBig2Games(nGames)\n",
    "        \n",
    "        #network/model for training\n",
    "        self.trainingNetwork =  Agent(input_dims=observation_space, env=self.vectorizedGame, n_actions=available_action_space, batch_size=nGames*nSteps)\n",
    "        \n",
    "        #player networks which choose decisions - allowing for later on experimenting with playing against older versions of the network (so decisions they make are not trained on).\n",
    "        self.playerNetworks = {}\n",
    "        \n",
    "        #for now each player uses the same (up to date) network to make it's decisions.\n",
    "        self.playerNetworks[1] = self.playerNetworks[2] = self.playerNetworks[3] = self.playerNetworks[4] = self.trainingNetwork\n",
    "        self.trainOnPlayer = [True, True, True, True]\n",
    "\n",
    "        #params\n",
    "        self.nGames = nGames\n",
    "        self.inpDim = inpDim\n",
    "        self.nSteps = nSteps\n",
    "        self.nMiniBatches = nMiniBatches\n",
    "        self.nOptEpochs = nOptEpochs\n",
    "        self.lam = lam\n",
    "        self.gamma = gamma\n",
    "        self.learningRate = learningRate\n",
    "        self.minLearningRate = minLearningRate\n",
    "        self.clipRange = clipRange\n",
    "        self.saveEvery = saveEvery\n",
    "        \n",
    "        self.rewardNormalization = 5.0 #divide rewards by this number (so reward ranges from -1.0 to 3.0)\n",
    "        \n",
    "        #test networks - keep network saved periodically and run test games against current network\n",
    "        self.testNetworks = {}\n",
    "        \n",
    "        # final 4 observations need to be carried over (for value estimation and propagating rewards back)\n",
    "        self.prevObs = []\n",
    "        self.prevGos = []\n",
    "        self.prevAvailAcs = []\n",
    "        self.prevRewards = []\n",
    "        self.prevActions = []\n",
    "        self.prevValues = []\n",
    "        self.prevDones = []\n",
    "        \n",
    "        #episode/training information\n",
    "        self.totTrainingSteps = 0\n",
    "        self.epInfos = []\n",
    "        self.gamesDone = 0\n",
    "        self.losses = []\n",
    "        \n",
    "    def run(self):\n",
    "        mb_individual_rewards = []\n",
    "        mb_obs, mb_pGos, mb_actions,  mb_neglogpacs, mb_rewards, mb_dones, mb_availAcs, mb_next_obs = [], [], [], [], [], [], [], []\n",
    "        print('appending prev_obs with size:',  len(self.prevObs))\n",
    "\n",
    "\n",
    "        # # appending previous observations\n",
    "        # for i in range(len(self.prevObs)):\n",
    "        #     mb_obs.append(self.prevObs[i])\n",
    "        #     mb_pGos.append(self.prevGos[i])\n",
    "        #     mb_actions.append(self.prevActions[i])\n",
    "        #     # mb_values.append(self.prevValues[i])\n",
    "        #     mb_neglogpacs.append(self.prevNeglogpacs[i])\n",
    "        #     mb_rewards.append(self.prevRewards[i])\n",
    "        #     mb_dones.append(self.prevDones[i])\n",
    "        #     mb_availAcs.append(self.prevAvailAcs[i])\n",
    "\n",
    "        # print('mb_obs: \n",
    "        # observations[games][step]\n",
    "\n",
    "        # observation[games * step]\n",
    "        if len(self.prevObs) == 4:\n",
    "            endLength = self.nSteps\n",
    "        else:\n",
    "            endLength = self.nSteps-4\n",
    "\n",
    "        print('running game for steps: ', self.nSteps)\n",
    "        #run vectorized games for nSteps and generate mini batch to train on.\n",
    "        for _ in range(self.nSteps):\n",
    "\n",
    "            # getting gos, states, and available actions from vectorized games\n",
    "            currGos, currStates, currAvailAcs = self.vectorizedGame.getCurrStates()\n",
    "            currStates = np.squeeze(currStates)\n",
    "            currAvailAcs = np.squeeze(currAvailAcs)\n",
    "            currGos = np.squeeze(currGos)\n",
    "            \n",
    "            # generate observatios from curstates and curavailacs\n",
    "            neglogpacs = self.trainingNetwork.choose_action(currStates)\n",
    "            \n",
    "            # add probabilities to the available actions, to rule out impossible actions\n",
    "            possible_actions_probablities = np.add(neglogpacs,  currAvailAcs)\n",
    "\n",
    "            print('possible_actions_probablities', possible_actions_probablities) \n",
    "            actions = np.argmax(possible_actions_probablities, axis=1)\n",
    "            print('actions taken: ' , actions)\n",
    "            \n",
    "            # step in the environment\n",
    "            rewards, dones, infos = self.vectorizedGame.step(actions)\n",
    "            print('rewards', rewards)\n",
    "           \n",
    "\n",
    "            # append to array initialized at the start\n",
    "            mb_obs.append(currStates.copy())\n",
    "            mb_pGos.append(currGos)\n",
    "            mb_availAcs.append(currAvailAcs.copy())\n",
    "            mb_actions.append(actions)\n",
    "            mb_neglogpacs.append(neglogpacs)\n",
    "            mb_dones.append(list(dones))\n",
    "            # mb_rewards.append(np.array(rewards))\n",
    "            currGos, currStates, currAvailAcs = self.vectorizedGame.getCurrStates()\n",
    "            mb_next_obs.append(currStates.copy())\n",
    "            \n",
    "            #now back assign rewards if state is terminal\n",
    "            toAppendRewards = np.ones((self.nGames,))\n",
    "            mb_rewards.append(toAppendRewards)\n",
    "\n",
    "            for i in range(self.nGames):\n",
    "                if dones[i] == True:\n",
    "                    print('finished game' , dones[i], rewards[i])\n",
    "                    reward = rewards[i]\n",
    "                    mb_rewards[-1][i] = reward[mb_pGos[-1][i]-1] / self.rewardNormalization\n",
    "                    mb_rewards[-2][i] = reward[mb_pGos[-2][i]-1] / self.rewardNormalization\n",
    "                    mb_rewards[-3][i] = reward[mb_pGos[-3][i]-1] / self.rewardNormalization\n",
    "                    mb_rewards[-4][i] = reward[mb_pGos[-4][i]-1] / self.rewardNormalization\n",
    "                    mb_dones[-2][i] = True\n",
    "                    mb_dones[-3][i] = True\n",
    "                    mb_dones[-4][i] = True\n",
    "                    self.epInfos.append(infos[i])\n",
    "                    self.gamesDone += 1\n",
    "            \n",
    "            \n",
    "        self.prevObs = mb_obs[endLength:]\n",
    "        self.prevGos = mb_pGos[endLength:]\n",
    "        self.prevRewards = mb_rewards[endLength:]\n",
    "        self.prevActions = mb_actions[endLength:]\n",
    "        # self.prevValues = mb_values[endLength:]\n",
    "        self.prevDones = mb_dones[endLength:]\n",
    "        self.prevNeglogpacs = mb_neglogpacs[endLength:]\n",
    "        self.prevAvailAcs = mb_availAcs[endLength:]\n",
    "\n",
    "\n",
    "        # convert back to numpy array\n",
    "        print('end length: ', endLength)\n",
    "        print('before cutting: ',  np.asarray(mb_obs, dtype=np.float32).shape)\n",
    "        mb_obs =  np.asarray(mb_obs, dtype=np.float32)\n",
    "        print('after cutting: ',mb_obs.shape)\n",
    "\n",
    "        mb_availAcs =  np.asarray(mb_availAcs, dtype=np.float32)\n",
    "        mb_rewards =  np.asarray(mb_rewards, dtype=np.float32)\n",
    "        mb_actions =   np.asarray(mb_actions, dtype=np.float32)\n",
    "        mb_next_obs =  np.asarray(mb_next_obs, dtype=np.float32)\n",
    "        mb_dones =  np.asarray(mb_dones, dtype=np.float32)\n",
    "        mb_individual_rewards =  np.asarray(mb_individual_rewards, dtype=np.float32)\n",
    "        \n",
    "\n",
    "        # mb_rewards[mb_rewards == 0] = 1\n",
    "        # print('mb_rewards', mb_rewards)\n",
    "        return map(sf01, (mb_obs, mb_availAcs, mb_rewards, mb_actions, mb_next_obs, mb_dones))\n",
    "        \n",
    "    def train(self, nTotalSteps):   \n",
    "        \n",
    "        nUpdates = nTotalSteps // (self.nGames * self.nSteps)\n",
    "\n",
    "        load_checkpoint = False\n",
    "        for update in range(nUpdates):\n",
    "            print('---------------------MINIBATCH %d -----------------'% (update+1))\n",
    "            \n",
    "            # get minibatch  \n",
    "            mb_obs, mb_availAcs, mb_rewards, mb_actions, mb_next_obs, mb_dones = self.run()\n",
    "            print('minibatch size' , self.nGames * self.nSteps)\n",
    "            print('minibatch produced -> ', mb_obs.shape, mb_availAcs.shape, mb_rewards.shape, mb_actions.shape, mb_next_obs.shape, mb_dones.shape)\n",
    "            # print('mb_returns', np.max(mb_rewards))\n",
    "            print('remembering minibatch')\n",
    "\n",
    "            for i in range(0, mb_obs.shape[0]):\n",
    "                self.trainingNetwork.remember(mb_obs[i], mb_actions[i], mb_rewards[i], mb_next_obs[i], mb_dones[i])\n",
    "            \n",
    "            if not load_checkpoint:\n",
    "                print('learning params')\n",
    "                self.trainingNetwork.learn()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 12 observations and 12 rewards\n",
    "\n",
    "    import time\n",
    "    old_stdout = sys.stdout\n",
    "\n",
    "    log_file = open(\"SAC.log\",\"w\") \n",
    "    sys.stdout = log_file\n",
    "\n",
    "    mainSim = big2PPOSimulation( nGames=3, nSteps=4, learningRate = 0.00025, clipRange = 0.2)\n",
    "    start = time.time()\n",
    "    mainSim.train(1000000)\n",
    "    end = time.time()\n",
    "    print(\"Time Taken: %f\" % (end-start))\n",
    "    sys.stdout = old_stdout\n",
    "\n",
    "    log_file.close()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8db0078cc9452821f9cd51677ee79e6eba5cdf5bbf7e94b8bd3e10960518e472"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('deeplearn_course': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
