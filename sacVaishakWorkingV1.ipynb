{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Soft Actor Critic Demystified</h1>\n",
    "<h4> By Vaishak Kumar </h4>\n",
    "<br>\n",
    "<a href=\"https://arxiv.org/pdf/1801.01290.pdf\">Original Paper</a>\n",
    "<br> \n",
    "<a href=\"https://github.com/higgsfield/RL-Adventure-2\">Adapted from higgsfield's implementation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from big2Game import vectorizedBig2Games\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Auxilliary Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.highnum_inputs, num_actions, \n",
    "        \n",
    "        action = 2 * (action - low) / (high - low) - 1\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Network Definitions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, init_w=3e-3):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        # print('num inputs0,')\n",
    "        # print('state shape', state.shape, 'action shape', action.shape)\n",
    "        # print('x.shape', x.shape)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.mean_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "        self.log_std_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(0, 1)\n",
    "        z      = normal.sample()\n",
    "        action = torch.tanh(mean+ std*z.to(device))\n",
    "        log_prob = Normal(mean, std).log_prob(mean+ std*z.to(device)) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        return action, log_prob, z, mean, log_std\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(0, 1)\n",
    "        z      = normal.sample().to(device)\n",
    "        action = torch.tanh(mean + std*z)\n",
    "        \n",
    "        action  = action.cpu()#.detach().cpu().numpy()\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Update Function </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(batch_size,gamma=0.99,soft_tau=1e-2,):\n",
    "    \n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "    done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "    predicted_q_value1 = soft_q_net1(state, action)\n",
    "    predicted_q_value2 = soft_q_net2(state, action)\n",
    "    predicted_value    = value_net(state)\n",
    "    new_action, log_prob, epsilon, mean, log_std = policy_net.evaluate(state)\n",
    "\n",
    "    \n",
    "    \n",
    "# Training Q Function\n",
    "    target_value = target_value_net(next_state)\n",
    "    target_q_value = reward + (1 - done) * gamma * target_value\n",
    "    q_value_loss1 = soft_q_criterion1(predicted_q_value1, target_q_value.detach())\n",
    "    q_value_loss2 = soft_q_criterion2(predicted_q_value2, target_q_value.detach())\n",
    "\n",
    "\n",
    "    soft_q_optimizer1.zero_grad()\n",
    "    q_value_loss1.backward()\n",
    "    soft_q_optimizer1.step()\n",
    "    soft_q_optimizer2.zero_grad()\n",
    "    q_value_loss2.backward()\n",
    "    soft_q_optimizer2.step()    \n",
    "# Training Value Function\n",
    "    predicted_new_q_value = torch.min(soft_q_net1(state, new_action),soft_q_net2(state, new_action))\n",
    "    target_value_func = predicted_new_q_value - log_prob\n",
    "    value_loss = value_criterion(predicted_value, target_value_func.detach())\n",
    "\n",
    "    \n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "# Training Policy Function\n",
    "    policy_loss = (log_prob - predicted_new_q_value).mean()\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    \n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Initializations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGames = 1\n",
    "env = vectorizedBig2Games(nGames)\n",
    "\n",
    "action_dim = 1695 # env.action_space.shape[0]\n",
    "state_dim  = 412 #env.observation_space.shape[0]\n",
    "hidden_dim = 256\n",
    "\n",
    "value_net        = ValueNetwork(state_dim, hidden_dim).to(device)\n",
    "target_value_net = ValueNetwork(state_dim, hidden_dim).to(device)\n",
    "\n",
    "soft_q_net1 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "soft_q_net2 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "\n",
    "value_criterion  = nn.MSELoss()\n",
    "soft_q_criterion1 = nn.MSELoss()\n",
    "soft_q_criterion2 = nn.MSELoss()\n",
    "\n",
    "value_lr  = 3e-4\n",
    "soft_q_lr = 3e-4\n",
    "policy_lr = 3e-4\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(), lr=value_lr)\n",
    "soft_q_optimizer1 = optim.Adam(soft_q_net1.parameters(), lr=soft_q_lr)\n",
    "soft_q_optimizer2 = optim.Adam(soft_q_net2.parameters(), lr=soft_q_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "\n",
    "replay_buffer_size = 1000000\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames  = 40000\n",
    "max_steps   = 500\n",
    "frame_idx   = 0\n",
    "rewards     = []\n",
    "batch_size  = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yosua/anaconda3/envs/fp_cs5446/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yosua/anaconda3/envs/fp_cs5446/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yosua/Documents/Projects/big2/big2Game.py\", line 533, in worker\n",
      "    reward, done, info = game.step(data)\n",
      "  File \"/home/yosua/Documents/Projects/big2/big2Game.py\", line 507, in step\n",
      "    opt, nC = enumerateOptions.getOptionNC(action)\n",
      "  File \"/home/yosua/Documents/Projects/big2/enumerateOptions.py\", line 23, in getOptionNC\n",
      "    if ind == passInd:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1694 , r  0.0\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6064/368621790.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mactoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;31m# print('action taken', action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/big2/big2Game.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcurrStates_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/big2/big2Game.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/big2/big2Game.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fp_cs5446/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fp_cs5446/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fp_cs5446/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"got end of file during message\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from math import inf\n",
    "while frame_idx < max_frames:\n",
    "    [playerTurn, state, availableActions] = env.getCurrStates()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        ## Run action for player 1\n",
    "        player1_reward = 0\n",
    "        # if frame_idx >1000:\n",
    "            [playerTurn, state, availableActions] = env.getCurrStates()\n",
    "            action = policy_net.get_action(state).detach().numpy()[0]\n",
    "            action = np.add(action, availableActions)\n",
    "            actoin = action.argmax()\n",
    "            reward, done, next_state = env.step([action])  \n",
    "            # print('action taken', action)\n",
    "\n",
    "            player1_reward += np.array(reward).squeeze()[0]\n",
    "            # print('reward', player1_reward)\n",
    "\n",
    "        else:\n",
    "            # action = env.action_space.sample()\n",
    "            action = random.randint(0,action_dim-1)\n",
    "          \n",
    "            while availableActions[0,0,action] == -inf:\n",
    "                action =  random.randint(0,action_dim-1)\n",
    "                \n",
    "            # print('action taken random', action)\n",
    "            # print('availableActions', availableActions[0,0,action])\n",
    "            reward, done, next_state = env.step([action])\n",
    "\n",
    "            player1_reward += np.array(reward).squeeze()[0]\n",
    "            # print('reward', player1_reward)\n",
    "\n",
    "        print('a', action, ', r ', player1_reward)\n",
    "        \n",
    "        # print('pushing to buffer', state, action, reward, next_state, done)\n",
    "        # print('pushing to buffer', np.array(state.shape), action,player1_reward, np.array(next_state).shape, np.array(done).shape)\n",
    "        # pushing to buffer [  1   1 412] () (1,) (1, 4) (1, 3)\n",
    "        # print('next_state', next_state)\n",
    "        state = state.squeeze()\n",
    "        next_state = np.array(next_state).squeeze()\n",
    "        action = [action]\n",
    "        done = done[0]\n",
    "        # print('pushing to buffer after shape', np.array(state).shape, action,player1_reward, np.array(next_state).shape, done)\n",
    "\n",
    "\n",
    "        # try one hot encoding the action space\n",
    "        action_one_hot = np.zeros(action_dim)\n",
    "        action_one_hot[action] = 1\n",
    "        action = action_one_hot\n",
    "        # print('one hot encode action', action)\n",
    "        # action = np.zeros(action_dim)[action] = 1 \n",
    "        replay_buffer.push(state, action, player1_reward, next_state, done)\n",
    "# pushing to buffer [ 0.29733867 -0.95477208 -0.69822459] [-1.6809958] -1.6628393972491333 [ 0.21450876 -0.97672206 -1.71430365] False\n",
    "# pushing to buffer after shape [412] [7] 0.0 (1, 1, 412) (1,)\n",
    "\n",
    "        ### Run action for player 2 - 4\n",
    "        ### The problem is that it doesnt take into acount the available actions\n",
    "        for player in range(1, 3):\n",
    "            [playerTurn, state, availableActions] = env.getCurrStates()\n",
    "            action = policy_net.get_action(state).detach().numpy()[0]\n",
    "            action1 = action\n",
    "            action = np.add(action, availableActions)\n",
    "            action2 = action\n",
    "            action = action[0,0,:].argmax()\n",
    "            # print('action taken opponent', action)\n",
    "            reward, done, next_state = env.step([action])            \n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += player1_reward\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            update(batch_size)\n",
    "            print('updating params')\n",
    "        \n",
    "        if frame_idx % 1000 == 0:\n",
    "            plot(frame_idx, rewards)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Visualize Trained Algorithm </h2> - <a href=\"http://mckinziebrandon.me/TensorflowNotebooks/2016/12/21/openai.html\">source</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(anim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageData' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_94751/3598216372.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Render into buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fp_cs5446/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fp_cs5446/lib/python3.7/site-packages/gym/envs/classic_control/pendulum.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_u\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fp_cs5446/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_buffer_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_color_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;31m# In https://github.com/openai/gym-http-api/issues/2, we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# discovered that someone using Xmonad on Arch was having\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageData' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "# Run a demo of the environment\n",
    "state = env.reset()\n",
    "cum_reward = 0\n",
    "frames = []\n",
    "for t in range(50000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    action = policy_net.get_action(state)\n",
    "    state, reward, done, info = env.step(action.detach())\n",
    "    if done:\n",
    "        break\n",
    "env.close()\n",
    "display_frames_as_gif(frames)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
