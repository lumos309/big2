{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_212578/2687018505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;31m# with tf.compat.v1.Session() as sess:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mmainSim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig2PPOSimulation\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnGames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnSteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipRange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mmainSim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_212578/2687018505.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inpDim, nGames, nSteps, nMiniBatches, nOptEpochs, lam, gamma, ent_coef, vf_coef, max_grad_norm, minLearningRate, learningRate, clipRange, saveEvery)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#network/model for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainingNetwork\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizedGame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavailable_action_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#player networks which choose decisions - allowing for later on experimenting with playing against older versions of the network (so decisions they make are not trained on).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/big2/sac_torch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alpha, beta, input_dims, env, gamma, n_actions, max_size, tau, layer1_size, layer2_size, batch_size, reward_scale)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_network_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/big2/sac_torch.py\u001b[0m in \u001b[0;36mupdate_network_parameters\u001b[0;34m(self, tau)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_state_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mvalue_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalue_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtarget_value_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sac_torch import Agent\n",
    "from big2Game import vectorizedBig2Games\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "#taken directly from baselines implementation - reshape minibatch in preparation for training.\n",
    "def sf01(arr):\n",
    "    \"\"\"\n",
    "    swap and then flatten axes 0 and 1\n",
    "    \"\"\"\n",
    "    s = arr.shape\n",
    "    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\n",
    "\n",
    "\n",
    "\n",
    "class big2PPOSimulation(object):\n",
    "    \n",
    "    def __init__(self, *, inpDim = 412, nGames = 4, nSteps = 100, nMiniBatches = 4, nOptEpochs = 5, lam = 0.95, gamma = 0.995, ent_coef = 0.01, vf_coef = 0.5, max_grad_norm = 0.5, minLearningRate = 0.000001, learningRate, clipRange, saveEvery = 500):\n",
    "         \n",
    "        available_action_space =  1695\n",
    "        observation_space = [412]\n",
    "        \n",
    "        #environment\n",
    "        self.vectorizedGame = vectorizedBig2Games(nGames)\n",
    "        \n",
    "        #network/model for training\n",
    "        self.trainingNetwork =  Agent(input_dims=observation_space, env=self.vectorizedGame, n_actions=available_action_space)\n",
    "        \n",
    "        #player networks which choose decisions - allowing for later on experimenting with playing against older versions of the network (so decisions they make are not trained on).\n",
    "        self.playerNetworks = {}\n",
    "        \n",
    "        #for now each player uses the same (up to date) network to make it's decisions.\n",
    "        self.playerNetworks[1] = self.playerNetworks[2] = self.playerNetworks[3] = self.playerNetworks[4] = self.trainingNetwork\n",
    "        self.trainOnPlayer = [True, True, True, True]\n",
    "\n",
    "        #params\n",
    "        self.nGames = nGames\n",
    "        self.inpDim = inpDim\n",
    "        self.nSteps = nSteps\n",
    "        self.nMiniBatches = nMiniBatches\n",
    "        self.nOptEpochs = nOptEpochs\n",
    "        self.lam = lam\n",
    "        self.gamma = gamma\n",
    "        self.learningRate = learningRate\n",
    "        self.minLearningRate = minLearningRate\n",
    "        self.clipRange = clipRange\n",
    "        self.saveEvery = saveEvery\n",
    "        \n",
    "        self.rewardNormalization = 5.0 #divide rewards by this number (so reward ranges from -1.0 to 3.0)\n",
    "        \n",
    "        #test networks - keep network saved periodically and run test games against current network\n",
    "        self.testNetworks = {}\n",
    "        \n",
    "        # final 4 observations need to be carried over (for value estimation and propagating rewards back)\n",
    "        self.prevObs = []\n",
    "        self.prevGos = []\n",
    "        self.prevAvailAcs = []\n",
    "        self.prevRewards = []\n",
    "        self.prevActions = []\n",
    "        self.prevValues = []\n",
    "        self.prevDones = []\n",
    "        \n",
    "        #episode/training information\n",
    "        self.totTrainingSteps = 0\n",
    "        self.epInfos = []\n",
    "        self.gamesDone = 0\n",
    "        self.losses = []\n",
    "        \n",
    "    def run(self):\n",
    "        mb_individual_rewards = []\n",
    "        #run vectorized games for nSteps and generate mini batch to train on.\n",
    "        mb_obs, mb_pGos, mb_actions,  mb_neglogpacs, mb_rewards, mb_dones, mb_availAcs, mb_next_obs = [], [], [], [], [], [], [], []\n",
    "        for i in range(len(self.prevObs)):\n",
    "            mb_obs.append(self.prevObs[i])\n",
    "            mb_pGos.append(self.prevGos[i])\n",
    "            mb_actions.append(self.prevActions[i])\n",
    "            # mb_values.append(self.prevValues[i])\n",
    "            mb_neglogpacs.append(self.prevNeglogpacs[i])\n",
    "            mb_rewards.append(self.prevRewards[i])\n",
    "            mb_dones.append(self.prevDones[i])\n",
    "            mb_availAcs.append(self.prevAvailAcs[i])\n",
    "        if len(self.prevObs) == 4:\n",
    "            endLength = self.nSteps\n",
    "        else:\n",
    "            endLength = self.nSteps-4\n",
    "        for _ in range(self.nSteps):\n",
    "            currGos, currStates, currAvailAcs = self.vectorizedGame.getCurrStates()\n",
    "            currStates = np.squeeze(currStates)\n",
    "            currAvailAcs = np.squeeze(currAvailAcs)\n",
    "            currGos = np.squeeze(currGos)\n",
    "            \n",
    "            # generate observatios from curstates and curavailacs\n",
    "            neglogpacs = self.trainingNetwork.choose_action(currStates)\n",
    "            \n",
    "            # add probabilities to the available actions, to rule out impossible actions\n",
    "            possible_actions_probablities = np.add(neglogpacs,  currAvailAcs)\n",
    "            actions = np.argmax(possible_actions_probablities, axis=1)\n",
    "            print('actions taken: ' , actions)\n",
    "            \n",
    "            # step in the environment\n",
    "            rewards, dones, infos = self.vectorizedGame.step(actions)\n",
    "            # print('rewards', rewards)\n",
    "            \n",
    "            # add to the previous observations\n",
    "            \n",
    "            \n",
    "            # if (dones[0] == True):\n",
    "            #     print('results', rewards[0], dones[0], infos[0])\n",
    "            mb_obs.append(currStates.copy())\n",
    "            mb_pGos.append(currGos)\n",
    "            mb_availAcs.append(currAvailAcs.copy())\n",
    "            mb_actions.append(actions)\n",
    "            mb_neglogpacs.append(neglogpacs)\n",
    "            mb_dones.append(list(dones))\n",
    "            mb_rewards.append(np.array(rewards))\n",
    "            currGos, currStates, currAvailAcs = self.vectorizedGame.getCurrStates()\n",
    "            # print('currGos', currGos[0])\n",
    "            mb_next_obs.append(currStates.copy())\n",
    "            \n",
    "            #now back assign rewards if state is terminal\n",
    "            toAppendRewards = np.zeros((self.nGames,))\n",
    "            mb_rewards.append(toAppendRewards)\n",
    "            for i in range(self.nGames):\n",
    "                if dones[i] == True:\n",
    "                    print('finished game' , dones[i], rewards[i])\n",
    "                    reward = rewards[i]\n",
    "                    mb_rewards[-1][i] = reward[mb_pGos[-1][i]-1] / self.rewardNormalization\n",
    "                    mb_rewards[-2][i] = reward[mb_pGos[-2][i]-1] / self.rewardNormalization\n",
    "                    mb_rewards[-3][i] = reward[mb_pGos[-3][i]-1] / self.rewardNormalization\n",
    "                    mb_rewards[-4][i] = reward[mb_pGos[-4][i]-1] / self.rewardNormalization\n",
    "                    mb_dones[-2][i] = True\n",
    "                    mb_dones[-3][i] = True\n",
    "                    mb_dones[-4][i] = True\n",
    "                    self.epInfos.append(infos[i])\n",
    "                    self.gamesDone += 1\n",
    "                    # print(\"Game %d finished.    1Lasted %d turns\" % (self.gamesDone, infos[i]['numTurns']))\n",
    "            \n",
    "            \n",
    "        self.prevObs = mb_obs[endLength:]\n",
    "        self.prevGos = mb_pGos[endLength:]\n",
    "        self.prevRewards = mb_rewards[endLength:]\n",
    "        self.prevActions = mb_actions[endLength:]\n",
    "        # self.prevValues = mb_values[endLength:]\n",
    "        self.prevDones = mb_dones[endLength:]\n",
    "        self.prevNeglogpacs = mb_neglogpacs[endLength:]\n",
    "        self.prevAvailAcs = mb_availAcs[endLength:]\n",
    "            # print('rewards shape',  np.asarray(mb_rewards, dtype=np.float32))\n",
    "        mb_individual_rewards = np.zeros((self.nSteps, self.nGames))    \n",
    "        print('mb_rewards', mb_rewards)\n",
    "        # print('mb_rewards', np.asarray(mb_rewards, dtype=np.float32).shape)\n",
    "        for i in range(self.nGames):\n",
    "            for j in range(self.nSteps):\n",
    "                # mb_rewards[j][i] = mb_rewards[0][i][currGos[i]-1]\n",
    "                # print('mb_rewards', mb_rewards[0][i])\n",
    "                # print('asdasd',  mb_rewards[0][i][currGos[i]-1])\n",
    "                # print('currgos', currGos[i])\n",
    "                continue\n",
    "                mb_individual_rewards[j][i] =  np.max(mb_rewards[j][i][currGos[i]-1])\n",
    "                # mb_individual_rewards[j][i] =  mb_rewards[j][i][currGos[i]-1]\n",
    "                \n",
    "        mb_obs =  np.asarray(mb_obs, dtype=np.float32)[:endLength]\n",
    "        mb_availAcs =  np.asarray(mb_availAcs, dtype=np.float32)[:endLength]\n",
    "        # mb_rewards =  np.asarray(mb_rewards, dtype=np.float32)\n",
    "        mb_actions =   np.asarray(mb_actions, dtype=np.float32)[:endLength]\n",
    "        mb_next_obs =  np.asarray(mb_next_obs, dtype=np.float32)\n",
    "        mb_dones =  np.asarray(mb_dones, dtype=np.float32)[:endLength]\n",
    "        mb_individual_rewards =  np.asarray(mb_individual_rewards, dtype=np.float32)[:endLength]\n",
    "\n",
    "        # print('mb_rewards', mb_rewards)\n",
    "        return map(sf01, (mb_obs, mb_availAcs, mb_individual_rewards, mb_actions, mb_next_obs, mb_dones))\n",
    "        \n",
    "    def train(self, nTotalSteps):   \n",
    "\n",
    "        # available_action_space =  1695\n",
    "        # observation_space = [412]\n",
    "        # nGames = 8 \n",
    "        # gamesDone = 0\n",
    "        # n_step = 250\n",
    "        # filename = 'big2.png'\n",
    "        # figure_file = 'plots/' + filename\n",
    "        \n",
    "        # best_score = 0\n",
    "        # score_history = []\n",
    "        # load_checkpoint = False\n",
    "\n",
    "\n",
    "        # env = vectorizedBig2Games(nGames)\n",
    "        \n",
    "        \n",
    "        # # self play\n",
    "        # oppponents = {}\n",
    "        # agent = Agent(input_dims=observation_space, env=env,\n",
    "        #         n_actions=available_action_space)\n",
    "        # oppponents[1] = oppponents[2] = oppponents[3] = agent\n",
    "\n",
    "\n",
    "        # if load_checkpoint:\n",
    "        #     agent.load_models()\n",
    "        #     env.render(mode='human')\n",
    "\n",
    "        # for i in range(n_step):\n",
    "        #     observation = env.reset()\n",
    "        #     done = False\n",
    "        #     score = 0\n",
    "        #     while not done:\n",
    "        #         action = agent.choose_action(observation)\n",
    "        #         observation_, reward, done, info = env.step(action)\n",
    "                \n",
    "        #         score += reward\n",
    "        #         agent.remember(observation, action, reward, observation_, done)\n",
    "                \n",
    "        #         if not load_checkpoint:\n",
    "        #             agent.learn()\n",
    "                    \n",
    "        #         observation = observation_\n",
    "                \n",
    "                \n",
    "                \n",
    "        #     score_history.append(score)\n",
    "        #     avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        #     if avg_score > best_score:\n",
    "        #         best_score = avg_score\n",
    "        #         if not load_checkpoint:\n",
    "        #             agent.save_models()\n",
    "\n",
    "        #     print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\n",
    "\n",
    "        # if not load_checkpoint:\n",
    "        #     x = [i+1 for i in range(n_step)]\n",
    "        #     plot_learning_curve(x, score_history, figure_file)\n",
    "        \n",
    "        # --------------------------------------------------\n",
    "        \n",
    "        \n",
    "        nUpdates = nTotalSteps // (self.nGames * self.nSteps)\n",
    "        best_score = 0\n",
    "        score_history = []\n",
    "        load_checkpoint = False\n",
    "        for update in range(nUpdates):\n",
    "            \n",
    "            # get minibatch  \n",
    "            mb_obs, mb_availAcs, mb_rewards, mb_actions, mb_next_obs, mb_dones = self.run()\n",
    "            print('minibatch', mb_obs.shape, mb_availAcs.shape, mb_rewards.shape, mb_actions.shape, mb_next_obs.shape, mb_dones.shape)\n",
    "            # print('mb_returns', np.max(mb_rewards))\n",
    "            for i in range(mb_obs.shape[0]):\n",
    "                self.trainingNetwork.remember(mb_obs[i], mb_actions[i], mb_rewards[i], mb_next_obs[i], mb_dones[i])\n",
    "            \n",
    "            if not load_checkpoint:\n",
    "                self.trainingNetwork.learn()\n",
    "            # batchSize = states.shape[0]\n",
    "            # self.totTrainingSteps += batchSize\n",
    "            \n",
    "            # nTrainingBatch = batchSize // self.nMiniBatches\n",
    "            \n",
    "            # score_history.append(score)\n",
    "            # avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "            # if avg_score > best_score:\n",
    "            #     best_score = avg_score\n",
    "            #     if not load_checkpoint:\n",
    "            #         self.trainingNetwork.save_models()\n",
    "\n",
    "            # print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\n",
    "            # do the learning here\n",
    "            \n",
    "            # currParams = self.trainingNetwork.getParams()\n",
    "            \n",
    "            # mb_lossvals = []\n",
    "            # inds = np.arange(batchSize)\n",
    "            # for _ in range(self.nOptEpochs):\n",
    "            #     np.random.shuffle(inds)\n",
    "            #     for start in range(0, batchSize, nTrainingBatch):\n",
    "            #         end = start + nTrainingBatch\n",
    "            #         mb_inds = inds[start:end]\n",
    "            #         mb_lossvals.append(self.trainingModel.train(lrnow, cliprangenow, states[mb_inds], availAcs[mb_inds], returns[mb_inds], actions[mb_inds], values[mb_inds], neglogpacs[mb_inds]))\n",
    "            # lossvals = np.mean(mb_lossvals, axis=0)\n",
    "            # self.losses.append(lossvals)\n",
    "            \n",
    "            # newParams = self.trainingNetwork.getParams()\n",
    "            # needToReset = 0\n",
    "            # for param in newParams:\n",
    "            #     if np.sum(np.isnan(param)) > 0:\n",
    "            #         needToReset = 1\n",
    "                    \n",
    "            # if needToReset == 1:\n",
    "            #     self.trainingNetwork.loadParams(currParams)\n",
    "            # print(update)\n",
    "            # if update % self.saveEvery == 0:\n",
    "            #     name = \"modelParameters\" + str(update)\n",
    "            #     self.trainingNetwork.saveParams(name)\n",
    "            #     joblib.dump(self.losses,\"losses.pkl\")\n",
    "            #     joblib.dump(self.epInfos, \"epInfos.pkl\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    old_stdout = sys.stdout\n",
    "\n",
    "    log_file = open(\"SAC.log\",\"w\")\n",
    "    sys.stdout = log_file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # with tf.compat.v1.Session() as sess:\n",
    "    mainSim = big2PPOSimulation( nGames=64, nSteps=20, learningRate = 0.00025, clipRange = 0.2)\n",
    "    start = time.time()\n",
    "    mainSim.train(1000000)\n",
    "    end = time.time()\n",
    "    print(\"Time Taken: %f\" % (end-start))\n",
    "    sys.stdout = old_stdout\n",
    "\n",
    "    log_file.close()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77f49737d0bbffe5816493168833c57c149a3ae0a3875787b4fa8a3eac35d10e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('deeplearn_course': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
